#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å­¦æœ¯Agent LLMå¢å¼ºåŠŸèƒ½æµ‹è¯•
"""

from academic_agent.services import LocalAcademicService
from academic_agent.llm import get_llm_adapter
from academic_agent.qa import LLMEnhancedResearchModule
from academic_agent.config import load_config


def test_llm_features():
    """æµ‹è¯•LLMå¢å¼ºåŠŸèƒ½"""
    
    # åŠ è½½é…ç½®
    config = load_config()
    llm_config = config.get("llm", {}).get("zhipu", {})
    
    # åˆå§‹åŒ–æœåŠ¡
    service = LocalAcademicService(adapter_name="openalex")
    
    # åˆå§‹åŒ–LLM
    llm_adapter = get_llm_adapter("zhipu", {
        "api_key": llm_config.get("api_key"),
        "model_name": llm_config.get("model", "qwen3-max"),
        "base_url": llm_config.get("base_url"),
        "temperature": llm_config.get("temperature", 0.7),
        "max_tokens": llm_config.get("max_tokens", 2000)
    })
    
    # åˆå§‹åŒ–LLMå¢å¼ºæ¨¡å—
    llm_module = LLMEnhancedResearchModule(
        adapter=service.adapter,
        llm_adapter=llm_adapter
    )
    
    print("=" * 80)
    print("å­¦æœ¯Agent LLMå¢å¼ºåŠŸèƒ½æµ‹è¯•")
    print("=" * 80)
    
    # é—®é¢˜1: LLMå¢å¼º - æ™ºèƒ½è®ºæ–‡æ€»ç»“
    print("\n" + "=" * 80)
    print("é—®é¢˜1: è¯·æ€»ç»“GPT-4ç›¸å…³çš„ç ”ç©¶ï¼ˆä½¿ç”¨LLMåˆ†æï¼‰")
    print("=" * 80)
    
    try:
        # å…ˆæœç´¢GPT-4ç›¸å…³è®ºæ–‡
        papers = service.search_papers(
            keyword="GPT-4",
            start_year=2023,
            page_size=2
        )
        
        if papers['papers']:
            print(f"\næ‰¾åˆ° {len(papers['papers'])} ç¯‡è®ºæ–‡ï¼Œæ­£åœ¨ä½¿ç”¨LLMè¿›è¡Œåˆ†æ...\n")
            
            # ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æ€»ç»“
            result = llm_module.handle({
                "type": "smart_summary",
                "paper_ids": [p['paper_id'] for p in papers['papers']]
            })
            
            if result.get('code') == 200:
                print(f"ğŸ“Š è®ºæ–‡æ•°é‡: {result['data'].get('papers_count', 0)}")
                print(f"ğŸ¤– åˆ†ææ¨¡å‹: {result['data'].get('model', 'N/A')}")
                print(f"\nğŸ“ æ™ºèƒ½æ€»ç»“:\n")
                print(result['data'].get('summary', 'æš‚æ— æ€»ç»“'))
            else:
                print(f"âŒ LLMåˆ†æå¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")
        else:
            print("âŒ æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
    
    # é—®é¢˜2: LLMå¢å¼º - ç ”ç©¶è¶‹åŠ¿åˆ†æ
    print("\n" + "=" * 80)
    print("é—®é¢˜2: å¤§è¯­è¨€æ¨¡å‹é¢†åŸŸçš„ç ”ç©¶è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆä½¿ç”¨LLMåˆ†æï¼‰")
    print("=" * 80)
    
    try:
        print(f"\næ­£åœ¨æœç´¢å¤§è¯­è¨€æ¨¡å‹ç›¸å…³è®ºæ–‡å¹¶ä½¿ç”¨LLMè¿›è¡Œåˆ†æ...\n")
        
        result = llm_module.handle({
            "type": "research_trend_analysis",
            "keyword": "large language model",
            "start_year": 2023,
            "end_year": 2024
        })
        
        if result.get('code') == 200:
            print(f"ğŸ“Š åˆ†ææ—¶é—´èŒƒå›´: {result['data'].get('period', 'N/A')}")
            print(f"ğŸ“„ è®ºæ–‡æ•°é‡: {result['data'].get('papers_count', 0)}")
            print(f"ğŸ¤– åˆ†ææ¨¡å‹: {result['data'].get('model', 'N/A')}")
            print(f"\nğŸ“ˆ è¶‹åŠ¿åˆ†æ:\n")
            print(result['data'].get('trend_analysis', 'æš‚æ— åˆ†æ'))
        else:
            print(f"âŒ LLMåˆ†æå¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
    
    print("\n" + "=" * 80)
    print("LLMå¢å¼ºåŠŸèƒ½æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    test_llm_features()
