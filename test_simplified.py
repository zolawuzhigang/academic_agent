#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å­¦æœ¯Agentç®€åŒ–æµ‹è¯•è„šæœ¬
"""

from academic_agent.services import LocalAcademicService
from academic_agent.llm import get_llm_adapter
from academic_agent.qa import LLMEnhancedResearchModule
from academic_agent.config import load_config


def test_simplified():
    """ç®€åŒ–ç‰ˆæµ‹è¯•"""
    
    # åŠ è½½é…ç½®
    config = load_config()
    llm_config = config.get("llm", {}).get("zhipu", {})
    
    # åˆå§‹åŒ–æœåŠ¡
    service = LocalAcademicService(adapter_name="openalex")
    
    # åˆå§‹åŒ–LLM
    llm_adapter = get_llm_adapter("zhipu", {
        "api_key": llm_config.get("api_key"),
        "model_name": llm_config.get("model", "qwen3-max"),
        "base_url": llm_config.get("base_url"),
        "temperature": llm_config.get("temperature", 0.7),
        "max_tokens": llm_config.get("max_tokens", 2000)
    })
    
    # åˆå§‹åŒ–LLMå¢å¼ºæ¨¡å—
    llm_module = LLMEnhancedResearchModule(
        adapter=service.adapter,
        llm_adapter=llm_adapter
    )
    
    print("=" * 80)
    print("å­¦æœ¯AgentåŠŸèƒ½æµ‹è¯•ï¼ˆç®€åŒ–ç‰ˆï¼‰")
    print("=" * 80)
    
    # é—®é¢˜1: åŸºç¡€æŸ¥è¯¢ - æœç´¢æœºå™¨å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è®ºæ–‡
    print("\n" + "=" * 80)
    print("é—®é¢˜1: 2024å¹´æœºå™¨å­¦ä¹ é¢†åŸŸæœ‰å“ªäº›é«˜è¢«å¼•è®ºæ–‡ï¼Ÿ")
    print("=" * 80)
    
    try:
        results = service.search_papers(
            keyword="machine learning",
            start_year=2024,
            page_size=3
        )
        
        print(f"\næ‰¾åˆ° {results['total']} ç¯‡è®ºæ–‡ï¼Œä»¥ä¸‹æ˜¯å‰3ç¯‡ï¼š\n")
        for i, paper in enumerate(results['papers'], 1):
            authors = [a.get('name', 'Unknown') if isinstance(a, dict) else str(a) 
                      for a in paper.get('authors', [])[:3]]
            print(f"{i}. {paper['title']}")
            print(f"   ä½œè€…: {', '.join(authors)}")
            print(f"   è¢«å¼•: {paper.get('citations', 0)}")
            print(f"   æœŸåˆŠ: {paper.get('journal', 'N/A')}")
            print(f"   å¹´ä»½: {paper.get('publish_year', 'N/A')}")
            print()
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
    
    # é—®é¢˜2: LLMå¢å¼º - æ™ºèƒ½è®ºæ–‡æ€»ç»“
    print("\n" + "=" * 80)
    print("é—®é¢˜2: è¯·æ€»ç»“Transformerç›¸å…³çš„ç ”ç©¶è¿›å±•ï¼ˆä½¿ç”¨LLMåˆ†æï¼‰")
    print("=" * 80)
    
    try:
        # å…ˆæœç´¢Transformerç›¸å…³è®ºæ–‡
        papers = service.search_papers(
            keyword="transformer architecture",
            start_year=2023,
            page_size=2
        )
        
        if papers['papers']:
            print(f"\næ‰¾åˆ° {len(papers['papers'])} ç¯‡è®ºæ–‡ï¼Œæ­£åœ¨ä½¿ç”¨LLMè¿›è¡Œåˆ†æ...\n")
            
            # ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½æ€»ç»“
            result = llm_module.handle({
                "type": "smart_summary",
                "paper_ids": [p['paper_id'] for p in papers['papers']]
            })
            
            if result.get('code') == 200:
                print(f"ğŸ“Š è®ºæ–‡æ•°é‡: {result['data'].get('papers_count', 0)}")
                print(f"ğŸ¤– åˆ†ææ¨¡å‹: {result['data'].get('model', 'N/A')}")
                print(f"\nğŸ“ æ™ºèƒ½æ€»ç»“:\n")
                print(result['data'].get('summary', 'æš‚æ— æ€»ç»“'))
            else:
                print(f"âŒ LLMåˆ†æå¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")
        else:
            print("âŒ æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
    
    # é—®é¢˜3: è®ºæ–‡è¯¦æƒ…æŸ¥è¯¢
    print("\n" + "=" * 80)
    print("é—®é¢˜3: æŸ¥è¯¢Attention Is All You Needè¿™ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯")
    print("=" * 80)
    
    try:
        paper_id = "W2626778328"
        paper_info = service.get_paper_info(paper_id)
        
        if paper_info:
            print(f"\nè®ºæ–‡æ ‡é¢˜: {paper_info.get('title', 'N/A')}")
            print(f"å‘è¡¨å¹´ä»½: {paper_info.get('publish_year', 'N/A')}")
            print(f"è¢«å¼•æ¬¡æ•°: {paper_info.get('citations', 0)}")
            print(f"æœŸåˆŠ: {paper_info.get('journal', 'N/A')}")
            
            authors = [a.get('name', 'N/A') if isinstance(a, dict) else str(a) 
                      for a in paper_info.get('authors', [])[:5]]
            print(f"ä½œè€…: {', '.join(authors)}")
            
            keywords = paper_info.get('keywords', [])
            if keywords:
                print(f"å…³é”®è¯: {', '.join(keywords[:5])}")
            
            abstract = paper_info.get('abstract', '')
            if abstract:
                print(f"\næ‘˜è¦: {abstract[:300]}...")
        else:
            print("âŒ æœªæ‰¾åˆ°è¯¥è®ºæ–‡")
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
    
    # é—®é¢˜4: LLMå¢å¼º - ç ”ç©¶è¶‹åŠ¿åˆ†æ
    print("\n" + "=" * 80)
    print("é—®é¢˜4: å¤§è¯­è¨€æ¨¡å‹é¢†åŸŸçš„ç ”ç©¶è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆä½¿ç”¨LLMåˆ†æï¼‰")
    print("=" * 80)
    
    try:
        print(f"\næ­£åœ¨æœç´¢å¤§è¯­è¨€æ¨¡å‹ç›¸å…³è®ºæ–‡å¹¶ä½¿ç”¨LLMè¿›è¡Œåˆ†æ...\n")
        
        result = llm_module.handle({
            "type": "research_trend_analysis",
            "keyword": "large language model",
            "start_year": 2023,
            "end_year": 2024
        })
        
        if result.get('code') == 200:
            print(f"ğŸ“Š åˆ†ææ—¶é—´èŒƒå›´: {result['data'].get('period', 'N/A')}")
            print(f"ğŸ“„ è®ºæ–‡æ•°é‡: {result['data'].get('papers_count', 0)}")
            print(f"ğŸ¤– åˆ†ææ¨¡å‹: {result['data'].get('model', 'N/A')}")
            print(f"\nğŸ“ˆ è¶‹åŠ¿åˆ†æ:\n")
            print(result['data'].get('trend_analysis', 'æš‚æ— åˆ†æ'))
        else:
            print(f"âŒ LLMåˆ†æå¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
    
    print("\n" + "=" * 80)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    test_simplified()
